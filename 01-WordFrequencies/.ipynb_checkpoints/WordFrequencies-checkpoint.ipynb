{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Word Frequencies\n",
    "\n",
    "\n",
    "Within this notebook, we'll explore some text data and compile the top N most frequently occuring terms within categorical groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text dataset from SKlearn's [`fetch_20newsgroups`](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n",
    "\n",
    "SKlearn's [`fetch_20newsgroups`](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) is a pre-compiled dataset that (as its name entails) offers news data for 20 different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# define which categories we'd like to use\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.windows.x',\n",
    "    'misc.forsale',\n",
    "    'rec.autos',\n",
    "    'sci.space',\n",
    "    'rec.motorcycles',\n",
    "    'rec.sport.baseball',\n",
    "    'sci.crypt']\n",
    "\n",
    "news = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
    "                          categories=categories)\n",
    "# documents\n",
    "docs = news.data\n",
    "\n",
    "# categories\n",
    "cats = news.target\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame({\"body\": docs, \"category\": [news.target_names[x] for x in cats]})\n",
    "\n",
    "# how many documents per category?\n",
    "pd.value_counts(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas `apply` to broadcast the `split` function to every row's _body_ column\n",
    "\n",
    "Using the pandas `apply` function allows us to broadcast a function over all values of a particular column of a DataFrame (or a Series). Within the `apply` function, `lambda` is acting similar to a JavaScript arrow function. It is an abbreviated way to write a function. In the cell below, we're passing `x` as an argument and returning `x.split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_tokens'] = df['body'].apply(lambda x: x.split(x))\n",
    "df[['body','body_tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe by category and combine each record's list of clean tokens\n",
    "\n",
    "Performing a _sum_ aggregation on a column that contains lists will merge the lists into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df.groupby('category').agg({'body_tokens': 'sum'})\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the number of tokens by category\n",
    "\n",
    "Below we're computing 3 metrics:\n",
    "1. **Number of tokens** - Calculated by simply finding the length of each category's list of tokens\n",
    "2. **Number of _unique_ tokens** - Calculated by first reducing the list of tokens down to unique values using the `set` function, then finding the length\n",
    "3. **Lexical Diversity** - Defined as the ratio of unique terms to total terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_df = pd.DataFrame({\"Total Number of Tokens\": group_df['body_tokens'].apply(lambda x: len(x)),\n",
    "                        \"Number of Unique Tokens\": group_df['body_tokens'].apply(lambda x: len(set(x)))})\n",
    "\n",
    "explore_df[\"Lexical Diversity\"] = explore_df['Number of Unique Tokens'] / explore_df['Total Number of Tokens']\n",
    "\n",
    "explore_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Lexical Diversity\n",
    "\n",
    "Keep in mind that we don't know the origin of this data, or the number of authors that generated the underlying records, so conclusions based purely on the aggregate-level diversity scores may be skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_df['Lexical Diversity'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the frequencies of each term in the word lists and return the top n most frequent\n",
    "\n",
    "Below we're using the [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) function which receives an iterable object and returns a dictionary with each unique token's frequency. Then, we're using a combination of `sorted` and `operator.itemgetter` to perform a reverse sort on a dictionary by its values, as opposed to sorting by the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "def wordListToFreqList(wordlist, top_n=10):\n",
    "    \"\"\"Compile a list of all words and their frequency of occurence\"\"\"\n",
    "    \n",
    "    # count each term's number of occurrences\n",
    "    freqDict = Counter(wordlist)\n",
    "    \n",
    "    # sort the frequency dictionary by its values descending and return the items as a list of tuples\n",
    "    sortedFreqs = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return sortedFreqs[:top_n]\n",
    "\n",
    "freqs = group_df['body_tokens'].apply(lambda x: wordListToFreqList(x))\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the most frequently occurring terms for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gridspec allows us to dynamically add subplots in grid\n",
    "N = len(categories)\n",
    "cols = 2\n",
    "rows = int(math.ceil(N / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "\n",
    "# define the figure space for the plots\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(N*2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# iterate over number of categories to plot each one's top terms\n",
    "for i in range(N):\n",
    "    \n",
    "    # add a plot to the figure\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.set_title(f\"Most Frequent Words for: {categories[i]}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # break the terms and term counts into two lists/tuples\n",
    "    x,y = zip(*freqs[i])\n",
    "    \n",
    "    #plot the data\n",
    "    ax.bar(x,y)\n",
    "    \n",
    "    # place numeric label on the bar\n",
    "    for j, v in enumerate(y):\n",
    "        ax.text(j, v/2, str(v), color='white', fontweight='bold', ha='center')\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RicePythonData",
   "language": "python",
   "name": "ricepythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
